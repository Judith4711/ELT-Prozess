{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projekt 2:\n",
    "  \n",
    "1.)\tLaden Sie das Python-File ml_block_4_elt_1.ipynb  \n",
    "2.)\tAufgabe: Erläutern Sie das Python-File ml_block_4_elt_1.ipynb  \n",
    "3.)\tWo kommt der ELT-Prozess vor?  Sind wir hier flexibler wie bei dem ETL-Prozess?   \n",
    "4.)\tKann man den ELT-Prozess mit der Google Cloud erstellen?  \n",
    "  \n",
    "### Erstellen eines ELT-Prozesses  \n",
    "  \n",
    "Mit der CSV-Datei und Abspeichern der CSV-Datei in den Ordner raw  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas #Pandas installieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrahierte Daten gespeichert in web_logs.csv.\n",
      "Roh Daten\n",
      "   user_id  login_time  logout_time  activity\n",
      "0        1          10         30.0    browse\n",
      "1        2          20         50.0    search\n",
      "2        3          15         25.0    browse\n",
      "3        4          30          NaN    browse\n",
      "4        5          50         70.0  purchase\n",
      "5        6          70         90.0    browse\n",
      "6        7          40          NaN    search\n",
      "7        8          25         50.0    browse\n",
      "8        9          35         60.0  purchase\n",
      "9       10          45         70.0    search\n",
      "Rohdaten in 'raw/' gespeichert.\n",
      "Cleaned Data\n",
      "   user_id  login_time  logout_time  activity  session_duration\n",
      "0        1          10         30.0    browse              20.0\n",
      "1        2          20         50.0    search              30.0\n",
      "2        3          15         25.0    browse              10.0\n",
      "4        5          50         70.0  purchase              20.0\n",
      "5        6          70         90.0    browse              20.0\n",
      "7        8          25         50.0    browse              25.0\n",
      "8        9          35         60.0  purchase              25.0\n",
      "9       10          45         70.0    search              25.0\n",
      "Transformierte Daten gespeichert in transformed_web_logs.csv.\n",
      "ELT-Prozess erfolgreich abgeschlossen.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Extract\n",
    "# Dummy-Daten werden in ein Dictionary \"Web-Logs\" erstellt.\n",
    "\n",
    "web_logs = {\n",
    "    'user_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'login_time': [10, 20, 15, 30, 50, 70, 40, 25, 35, 45],\n",
    "    'logout_time': [30, 50, 25, None, 70, 90, None, 50, 60, 70],\n",
    "    'activity': ['browse', 'search', 'browse', 'browse', 'purchase', 'browse', 'search', 'browse', 'purchase', 'search']\n",
    "}\n",
    "\n",
    "# Daten speichern\n",
    "# Diese Daten werden in ein Pandas DataFrame umgewandelt und als CSV-Datei (web_logs.csv) gespeichert. \n",
    "# Dies simuliert den Extraktionsschritt, bei dem Rohdaten aus einer Quelle(hier: Dictionary) extrahiert werden und\n",
    "# in einem speicherbaren Format (hier: csv) organisiert werden\n",
    "web_logs_df = pd.DataFrame(web_logs)\n",
    "web_logs_csv = 'web_logs.csv'\n",
    "web_logs_df.to_csv(web_logs_csv, index=False)\n",
    "print(f\"Extrahierte Daten gespeichert in {web_logs_csv}.\")\n",
    "\n",
    "print(\"Roh Daten\")\n",
    "print(web_logs_df)\n",
    "\n",
    "\n",
    "# Step 2: Load (Raw Data)\n",
    "# Normalerweise würde hier ein Data Lake wie Amazon S3 verwendet werden.\n",
    "# Wir simulieren dies durch einfaches Speichern der Datei in einem Raw-Ordner.\n",
    "raw_data_path = 'raw/' # Ein Verzeichnis namens raw/ wird erstellt\n",
    "os.makedirs(raw_data_path, exist_ok=True)  # Ordner erstellen, falls nicht vorhanden\n",
    "\n",
    "# Der Code überprüft, ob die Datei bereits im raw/-Ordner existiert. \n",
    "# Wenn nicht, wird die CSV-Datei dorthin verschoben. \n",
    "# Dies stellt sicher, dass die Rohdaten an einem zentralen Ort gespeichert werden, ähnlich wie in einem Data Lake.\n",
    "destination_file = os.path.join(raw_data_path, web_logs_csv)\n",
    "if not os.path.exists(destination_file):\n",
    "    os.rename(web_logs_csv, destination_file)\n",
    "    print(f\"Rohdaten in '{raw_data_path}' gespeichert.\")\n",
    "else:\n",
    "    print(f\"Datei '{destination_file}' existiert bereits. Verschieben übersprungen.\")\n",
    "\n",
    "\n",
    "# Step 3: Transform (on-demand)\n",
    "# Die Rohdaten werden aus der CSV-Datei im raw/-Ordner geladen\n",
    "# Die Session-Dauer wird berechnet, indem die logout_time von der login_time subtrahiert wird. \n",
    "# Dies wird in einer neuen Spalte session_duration gespeichert.\n",
    "# Fehlende Werte (NaN) in der session_duration-Spalte werden entfernt, \n",
    "# um sicherzustellen, dass nur vollständige Datensätze für die weitere Analyse verwendet werden.\n",
    "raw_web_logs_path = os.path.join(raw_data_path, 'web_logs.csv')\n",
    "transformed_data_path = 'transformed_web_logs.csv'\n",
    "\n",
    "raw_data = pd.read_csv(raw_web_logs_path)\n",
    "raw_data['session_duration'] = raw_data['logout_time'] - raw_data['login_time'] \n",
    "cleaned_data = raw_data.dropna(subset=['session_duration'])\n",
    "\n",
    "print(\"Cleaned Data\")\n",
    "print(cleaned_data)\n",
    "\n",
    "# Transformierte Daten speichern\n",
    "# Die bereinigten und transformierten Daten werden in einer neuen CSV-Datei (transformed_web_logs.csv) gespeichert\n",
    "cleaned_data.to_csv(transformed_data_path, index=False)\n",
    "print(f\"Transformierte Daten gespeichert in {transformed_data_path}.\")\n",
    "\n",
    "# ELT-Prozess abgeschlossen\n",
    "print(\"ELT-Prozess erfolgreich abgeschlossen.\")\n",
    "\n",
    "# Anwendungsbeispiel: \n",
    "# Solch ein Prozess könnte in einem Web-Analyse-Tool verwendet werden, \n",
    "# um Benutzeraktivitäten zu verfolgen und zu analysieren, wie lange Benutzer auf einer Website aktiv sind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.)\ta) Wo kommt der ELT-Prozess vor?  b) Sind wir hier flexibler wie bei dem ETL-Prozess?   \n",
    "\n",
    "#### a) Typische Anwendungsfälle:\n",
    "\n",
    "**Big-Data-Analysen**:   \n",
    "Verarbeitung großer Datenmengen wie Kundendaten oder IoT-Sensordaten.\n",
    "\n",
    "**Echtzeit-Analysen**:   \n",
    "Live-Metriken für Marketingkampagnen\n",
    "\n",
    "**Datenlakes**:   \n",
    "Speicherung unstrukturierter Rohdaten für spätere Nutzung.    \n",
    "\n",
    "**KI/ML-Projekte**:  \n",
    "Nutzung roher Daten für Trainingsmodelle und maschinelles Lernen\n",
    "\n",
    "  \n",
    "    \n",
    "      \n",
    "  \n",
    "#### b) Ja, ELT-Prozesse sind flexibler als der traditionelle ETL-Prozess:\n",
    "* **Daten zuerst laden**: Rohdaten werden direkt in ein Zielsystem (z.B. Data Warehouse) geladen, bevor sie bearbeitet werden.  \n",
    "  \n",
    "* **Ad-hoc-Analysen**: Nutzer können sofort mit den Rohdaten arbeiten und bei Bedarf Anpassungen vornehmen.  \n",
    "  \n",
    "* **Skalierbarkeit**: Moderne Data Warehouses können große Datenmengen schnell verarbeiten, ohne vorherige Aufbereitung.  \n",
    "  \n",
    "* **Vielfältige Transformationen**: Nutzer können die Daten nach Bedarf anpassen, ohne sie erneut hochladen zu müssen.    \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.)\tKann man den ELT-Prozess mit der Google Cloud erstellen?   \n",
    "  \n",
    "**Extract**: Daten aus Quellen (z.B. Datenbank, CSV, API) holen → in Cloud Storage ablegen  (Speicher)\n",
    "  \n",
    "**Load**: Rohdaten in BigQuery laden (Data warehouse). LAden kann automatisch, per Zeitplan od. Echzeit erfolgen \n",
    "  \n",
    "**Transform**: Die Umwandlung der Daten findet in BigQuery statt. (zB Filtern, bereinigen, zusammen...)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WBS_2025_tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
